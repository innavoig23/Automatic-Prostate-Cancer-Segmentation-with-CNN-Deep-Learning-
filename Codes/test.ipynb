{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Hr2i-a4w8ysh"},"outputs":[],"source":["# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SwtNHfmH8sih"},"outputs":[],"source":["# Check nvcc version\n","!nvcc -V\n","# Check GCC version\n","!gcc --version\n","\n","# Install PyTorch\n","!pip install torch==1.11.0+cu113 torchvision==0.12.0+cu113 torchaudio==0.11.0+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html\n","# Install MMCV\n","!pip install mmcv==2.0.0rc4 -f https://download.openmmlab.com/mmcv/dist/cu113/torch1.11.0/index.html\n","# Install MMSegmentation\n","!pip install mmsegmentation\n","# Install MMEngine\n","!pip install mmengine\n","\n","# Install ftfy\n","!pip install ftfy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i8E4UmHT8sii"},"outputs":[],"source":["# Check Pytorch installation\n","import torch, torchvision\n","print(torch.__version__, torch.cuda.is_available())\n","\n","# Check MMSegmentation installation\n","import mmseg\n","print(mmseg.__version__)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e3wB3yI38sij"},"outputs":[],"source":["# Required imports definition\n","\n","# Standard Imports\n","import os\n","import numpy as np\n","import torch\n","\n","from mmseg.apis import init_model, inference_model\n","import matplotlib.pyplot as plt\n","\n","from PIL import Image\n","from tqdm import tqdm\n","\n","from mmengine import Config\n","\n","from mmseg.registry import DATASETS\n","from mmseg.datasets import BaseSegDataset\n","\n","# PreProcessing Class Imports\n","from skimage import util\n","import skimage.exposure as exposure\n","from scipy import ndimage\n","\n","from mmcv.transforms import BaseTransform, TRANSFORMS\n","\n","# Interpolation Imports\n","from scipy.ndimage import distance_transform_edt\n","\n","# Inference Imports\n","import torch.nn.functional as F"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l5xUrca1ITAM"},"outputs":[],"source":["# Include the name of your group\n","group_name = 'MS1'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bEDiW1Hb8sik"},"outputs":[],"source":["#################### INSTRUCTIONS FOR FOLDER MANAGEMENT ########################\n","#\n","# Put both config file and checkpoint in the same folder of your notebook\n","# Name config file as config.py\n","# Name checkpoint as checkpoint.pth\n","#\n","# N.B ::: Leave this as it is, we will input the required paths\n","#\n","\n","submission_folder = '...'\n","print(f'submission folder: {submission_folder}')\n","\n","checkpoint_path = os.path.join(submission_folder, 'checkpoint.pth')\n","cfg_path = os.path.join(submission_folder, 'config.py')\n","\n","test_img_folder = '...'\n","\n","results_folder = os.path.join(submission_folder, f'results_{group_name}')\n","\n","os.makedirs(results_folder, exist_ok=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W7TgYd3I8sil"},"outputs":[],"source":["classes = ('background',\n","           'tumor')\n","\n","paletteProstate = [\n","    (0, 0, 0),       # background - black\n","    (255, 255, 255), # tumor - white\n","]\n","\n","@DATASETS.register_module()\n","class ProstateMRI(BaseSegDataset):\n","    METAINFO = dict(classes = classes, palette = paletteProstate)\n","    def __init__(self, **kwargs):\n","        super().__init__(img_suffix='.png',\n","                        seg_map_suffix='.png',\n","                        reduce_zero_label = False,\n","                        **kwargs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZNtNqmBO8sil"},"outputs":[],"source":["# Load the config file and print to see if it's correct\n","cfg = Config.fromfile(cfg_path)\n","print(f'Config:\\n{cfg.pretty_text}')\n","\n","# Init the model from the config and the checkpoint\n","model = init_model(cfg, checkpoint_path, 'cuda:0')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Definition of pre-processing functions\n","\n","# Function to find black borders in T2W channel\n","def findBorders(t2w):\n","    # FIND BLACK BORDERS IN T2W CHANNEL\n","    # Make t2w boolean\n","    t2w_bool = t2w > 0\n","    # Find non-black rows and columns\n","    rows = np.any(t2w_bool, axis=1)\n","    cols = np.any(t2w_bool, axis=0)\n","    # Get first and last non-black row and column indexes\n","    rmin, rmax = np.where(rows)[0][[0, -1]] if np.any(rows) else (0, t2w.shape[0])\n","    cmin, cmax = np.where(cols)[0][[0, -1]] if np.any(cols) else (0, t2w.shape[1])\n","    return rmin, rmax, cmin, cmax\n","\n","# Function to crop and resize images according to black borders in T2W channel\n","def cropResizeImages(adc, hbv, t2w):\n","    # CROP IMAGES\n","    # Get original image size\n","    original_shape = t2w.shape\n","    # Find black borders in t2w channel\n","    rmin_t2w, rmax_t2w, cmin_t2w, cmax_t2w = findBorders(t2w)\n","    # Get final cropped image size\n","    cropDimension = min(rmax_t2w - rmin_t2w, cmax_t2w - cmin_t2w)\n","    if cropDimension % 2 != 0: # ensure crop_size is even\n","        cropDimension -= 1\n","    if abs(t2w.shape[0] - cropDimension) < 5: # do not crop if the black region is too small\n","        cropDimension = t2w.shape[0]\n","        flagCrop = False\n","        # resize images to 256x256\n","        resizeShape = (256, 256)\n","        adc_resized = np.array(Image.fromarray(adc, mode='L').resize(resizeShape, Image.LANCZOS), dtype=np.uint8)\n","        hbv_resized = np.array(Image.fromarray(hbv, mode='L').resize(resizeShape, Image.LANCZOS), dtype=np.uint8)\n","        t2w_resized = np.array(Image.fromarray(t2w, mode='L').resize(resizeShape, Image.LANCZOS), dtype=np.uint8)\n","        # Create the dictionary to store resize metadata\n","        cropMetadata = {'flagCrop': flagCrop, 'original_shape': original_shape, 'resizeShape': resizeShape, 'cropDimension': cropDimension}\n","        return adc_resized, hbv_resized, t2w_resized, cropMetadata\n","    else:\n","        flagCrop = True\n","\n","    # Get center of the non-black region\n","    center_x = (cmin_t2w + cmax_t2w) // 2\n","    center_y = (rmin_t2w + rmax_t2w) // 2\n","    # Get start and end coordinates for cropping\n","    start_x = max(center_x - cropDimension // 2, 0)\n","    start_y = max(center_y - cropDimension // 2, 0)\n","    end_x = min(start_x + cropDimension, t2w.shape[1])\n","    end_y = min(start_y + cropDimension, t2w.shape[0])\n","\n","    # Crop images at the center of the non-black region\n","    adc_cropped = adc[start_y:end_y, start_x:end_x]\n","    hbv_cropped = hbv[start_y:end_y, start_x:end_x]\n","    t2w_cropped = t2w[start_y:end_y, start_x:end_x]\n","\n","    cropped_shape = t2w_cropped.shape\n","\n","    # RESIZE IMAGES\n","    # Resize cropped images to 256x256\n","    resizeShape = (256, 256)\n","    adc_cropped_resized = np.array(Image.fromarray(adc_cropped, mode='L').resize(resizeShape, Image.LANCZOS), dtype=np.uint8)\n","    hbv_cropped_resized = np.array(Image.fromarray(hbv_cropped, mode='L').resize(resizeShape, Image.LANCZOS), dtype=np.uint8)\n","    t2w_cropped_resized = np.array(Image.fromarray(t2w_cropped, mode='L').resize(resizeShape, Image.LANCZOS), dtype=np.uint8)\n","\n","    # Create a dictionary to store crop metadata\n","    cropMetadata = {'start_x': start_x, 'start_y': start_y, 'end_x': end_x, 'end_y': end_y, 'cropDimension': cropDimension, 'cropped_shape': cropped_shape, 'center_x': center_x, 'center_y': center_y, 'original_shape': original_shape, 'resizeShape': resizeShape, 'flagCrop': flagCrop}\n","\n","    return adc_cropped_resized, hbv_cropped_resized, t2w_cropped_resized, cropMetadata\n","\n","# Function to restore the mask to the original size\n","def restoreSizeMask(mask, cropMetadata):\n","    # RESTORE AUTOMATIC MASK TO ORIGINAL SIZE\n","    # Check if the image was not cropped\n","    if not cropMetadata['flagCrop']:\n","        mask_restored = np.array(Image.fromarray(mask, mode='L').resize(cropMetadata['original_shape'], Image.NEAREST), dtype=np.uint8)\n","        return mask_restored\n","    # If the image was cropped, restore the mask to the original size\n","    else:\n","        mask_restored = np.zeros(cropMetadata['original_shape'], dtype=np.uint8)\n","        mask_cropDim = np.array(Image.fromarray(mask, mode='L').resize(cropMetadata['cropped_shape'], Image.NEAREST), dtype=np.uint8)\n","        mask_restored[cropMetadata['start_y']:cropMetadata['end_y'], cropMetadata['start_x']:cropMetadata['end_x']] = mask_cropDim\n","\n","        return mask_restored\n","\n","# Function to apply pre-processing to the images\n","def minMaxScaling(img):\n","    if np.max(img) != np.min(img):\n","        img = (img - np.min(img)) / (np.max(img) - np.min(img))\n","        img = np.array(img*255, dtype=np.uint8)\n","\n","    return img\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Definition of post-processing function\n","\n","# Function to apply interpolation between two masks\n","def interp_mask(mask1, mask2):\n","  # mask1 and mask2 must be two boolean numpy arrays that represent tumoral slices\n","  mask1 = np.array(mask1, dtype=bool)\n","  mask2 = np.array(mask2, dtype=bool)\n","  d1 = distance_transform_edt(mask1) - distance_transform_edt(~mask1)\n","  d2 = distance_transform_edt(mask2) - distance_transform_edt (~mask2)\n","  interpolated_mask = (d1 + d2) > 0\n","  interpolated_mask = np.array(interpolated_mask, dtype=np.uint8)\n","\n","  return interpolated_mask"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Get the list of patients\n","patientList = os.listdir(test_img_folder)\n","\n","# Loop over the test patients\n","for patient in tqdm(patientList):\n","\n","  # Create patient folder in the results folder\n","  os.makedirs(os.path.join(results_folder, patient, \"automatic\"), exist_ok=True)\n","\n","  # Get the list of patient's slices\n","  sliceList = os.listdir(os.path.join(test_img_folder, patient, \"adc\"))\n","\n","  # Initialization of elements needed for post-processing\n","  tumorVolume = 0\n","  numTumoralSlices = 0\n","  hasTumor = False\n","  listTumoralSlices = []\n","\n","  # Loop over the slices\n","  for slice in sliceList:\n","    if slice.endswith('.png'):\n","\n","      ###############################\n","      #######      Load      ########\n","      ###############################\n","\n","      adc_path = os.path.join(test_img_folder, patient, \"adc\", slice)\n","      hbv_path = os.path.join(test_img_folder, patient, \"hbv\", slice)\n","      t2w_path = os.path.join(test_img_folder, patient, \"t2w\", slice)\n","\n","      adc = np.array(Image.open(adc_path))\n","      hbv = np.array(Image.open(hbv_path))\n","      t2w = np.array(Image.open(t2w_path))\n","\n","      ###############################\n","      ####### Pre-Processing ########\n","      ###############################\n","\n","      # Crop\n","      adc, hbv, t2w, cropMetadata = cropResizeImages(adc, hbv, t2w)\n","\n","      # Min-Max Scaling\n","      adc = minMaxScaling(adc)\n","      hbv = minMaxScaling(hbv)\n","      t2w = minMaxScaling(t2w)\n","\n","      # Create multimodal RGB image\n","      stacked = np.stack((adc, hbv, t2w), axis=-1)\n","\n","      ###############################\n","      #######    Inference   ########\n","      ###############################\n","\n","      result = inference_model(model, stacked)\n","\n","      # Get data from the result\n","      # Logits extraction\n","      logitsScaleFactor = 0.5\n","      seg_logitsBG = logitsScaleFactor * result.seg_logits.data[0]\n","      seg_logitsTUM = logitsScaleFactor * result.seg_logits.data[1]\n","      seg_logits = torch.stack([seg_logitsBG, seg_logitsTUM], dim=0)\n","\n","      # Softmax Function Application (probability extraction)\n","      seg_probs = F.softmax(seg_logits, dim=0)\n","      seg_probs = seg_probs.cpu().numpy()\n","\n","      # Manual Probability Thresholding\n","      threshold = 0.6\n","      pred_label = (seg_probs[1] > threshold).astype(np.uint8)\n","\n","      ###############################\n","      ### Computation of elements ###\n","      ###       needed for        ###\n","      ###     post-processing     ###\n","      ###############################\n","\n","      # Computation of elements needed for post-processing\n","      if np.sum(pred_label) > 0:\n","          hasTumor = True\n","          numTumoralSlices += 1\n","          tumorVolume += np.sum(pred_label.astype(bool))\n","          listTumoralSlices.append(slice)\n","\n","      ###############################\n","      #######   Save results  #######\n","      ###############################\n","\n","      # Restore the original size\n","      pred_label = restoreSizeMask(pred_label, cropMetadata)\n","      pred_label = Image.fromarray(pred_label, mode='L')\n","\n","      # Save the result\n","      pred_label.save(os.path.join(results_folder, patient, \"automatic\", slice))\n","\n","\n","  ###############################\n","  ####### Post-Processing #######\n","  ###############################\n","\n","  # Check if the patient has a tumor: if so, apply post-processing techniques\n","  if hasTumor:\n","    # Extraction of the first slice of the patient to get the shape of the slices\n","    exampleSliceAdc = np.array(Image.open(os.path.join(test_img_folder, patient, \"adc\", sliceList[0])))\n","    shapeSlices = exampleSliceAdc.shape\n","\n","    # FIRST, SECOND, SECOND-LAST AND LAST SLICE CHECK: Check if first, second, second-last and/or last slices are tumoral and, if so, remove them from the list of tumoral slices and save black masks instead\n","    slicesToCheck = [f'slice_{i}.png' for i in [0, 1, len(sliceList) - 2, len(sliceList) - 1]]\n","    # Loop over the slices to check\n","    for actCheckingSlice in slicesToCheck:\n","      if actCheckingSlice in listTumoralSlices:\n","        # Create a black mask\n","        blackMask = Image.fromarray(np.zeros((shapeSlices[0], shapeSlices[1]), dtype=np.uint8)).convert('L')\n","        # Save the black mask\n","        blackMask.save(os.path.join(results_folder, patient, \"automatic\", actCheckingSlice))\n","        # Remove the slice from the list of tumoral slices\n","        listTumoralSlices.remove(actCheckingSlice)\n","\n","    # INTERPOLATION: Check if there are black slices between two tumoral slices: if so, interpolate all the black slices with the tumoral slices before and after\n","    maxBlackSlices = 3 # maximum number of black slices between two tumoral slices to interpolate: if there are more than 3 black slices, it coulde be a bad segmentation and so we do not interpolate\n","    listTumoralSlices.sort()\n","    # Extract the number of the tumoral slices from the list. They are called slice_n.png, so extract \"n\"\n","    listTumoralSlices = [int(slice.split('.')[0].split('_')[1]) for slice in listTumoralSlices]\n","    # Check how many black slices there are between two tumoral slices using difference between consecutive elements\n","    diffTumoralSlices = np.diff(listTumoralSlices) - 1\n","    # Loop over the differences\n","    for i in range(len(diffTumoralSlices)):\n","      if diffTumoralSlices[i] > 0:\n","        if diffTumoralSlices[i] <= maxBlackSlices:\n","          # Loop over the black slices between two tumoral slices\n","          for j in range(1, diffTumoralSlices[i]+1):\n","            # Get the top and bottom tumoral slices (called mask1 and mask2 in the function and in the report)\n","            top = np.array(Image.open(os.path.join(results_folder, patient, \"automatic\", f\"slice_{listTumoralSlices[i]}.png\")))\n","            bottom = np.array(Image.open(os.path.join(results_folder, patient, \"automatic\", f\"slice_{listTumoralSlices[i+1]}.png\")))\n","            # Interpolate the black slice with the tumoral slices\n","            black = interp_mask(top, bottom)\n","            black = Image.fromarray(black, mode='L')\n","            # Save the interpolated slice\n","            black.save(os.path.join(results_folder, patient, \"automatic\", f\"slice_{listTumoralSlices[i]+j}.png\"))"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.3"}},"nbformat":4,"nbformat_minor":0}
